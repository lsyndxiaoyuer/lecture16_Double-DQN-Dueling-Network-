{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/liangsy/opt/anaconda3/lib/python3.8/site-packages (4.5.2.52)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/liangsy/opt/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.19.5)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/liangsy/opt/anaconda3/lib/python3.8/site-packages (7.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 30000\n",
    "SHOW_EVERY = 3000\n",
    "\n",
    "epsilon = 0.6\n",
    "EPS_DECAY = 0.9998\n",
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cube:\n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, self.size)\n",
    "        self.y = np.random.randint(0, self.size)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.x},{self.y}'\n",
    "    \n",
    "    def __sub__(self,other):\n",
    "        return (self.x-other.x,self.y-other.y)\n",
    "    \n",
    "    def __eq__(self,other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "    def action(self,choise):\n",
    "        if choise == 0 :\n",
    "            self.move(x=1, y=1)\n",
    "        elif choise == 1 :\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choise == 2 :\n",
    "            self.move(x=1, y=-1)\n",
    "        elif choise == 3 :\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choise == 4 :\n",
    "            self.move(x=0, y=1)        \n",
    "        elif choise == 5 :\n",
    "            self.move(x=0, y=-1) \n",
    "        elif choise == 6 :\n",
    "            self.move(x=1, y=0) \n",
    "        elif choise == 7 :\n",
    "            self.move(x=-1, y=0)             \n",
    "        elif choise == 8 :\n",
    "            self.move(x=0, y=0)             \n",
    "            \n",
    "    def move(self,x=False,y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1,2)\n",
    "        else:\n",
    "            self.x += x\n",
    "            \n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1,2)\n",
    "        else:\n",
    "            self.y += y   \n",
    "            \n",
    "        if self.x < 0 :\n",
    "            self.x = 0\n",
    "        elif self.x >= self.size :\n",
    "            self.x = self.size -1\n",
    "\n",
    "        if self.y < 0 :\n",
    "            self.y = 0\n",
    "        elif self.y >= self.size :\n",
    "            self.y = self.size -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class envCube:\n",
    "    SIZE = 10\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE,SIZE,3)\n",
    "#     OBSERVATION_SPACE_VALUES = (4,)\n",
    "    ACTION_SPACE_VALUES = 9\n",
    "    RETURN_IMAGE = True\n",
    "    \n",
    "    FOOD_REWARD = 25\n",
    "    ENEMY_PENALITY = -300\n",
    "    MOVE_PENALITY = -1    \n",
    "    \n",
    "    d = {1:(255,0,0), #blue\n",
    "         2:(0,255,0), #green\n",
    "         3:(0,0,255)} #red\n",
    "\n",
    "    PLAYER_N = 1\n",
    "    FOOD_N =2\n",
    "    ENEMY_N =3    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.player = Cube(self.SIZE)\n",
    "        self.food = Cube(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Cube(self.SIZE)\n",
    "        \n",
    "        self.enemy = Cube(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Cube(self.SIZE)\n",
    "        \n",
    "        if self.RETURN_IMAGE:\n",
    "            observation = np.array(self.get_image())/255\n",
    "        else:\n",
    "            observation = (self.player - self.food)+(self.player - self.enemy) \n",
    "        \n",
    "        self.episode_step = 0\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "        self.food.move()\n",
    "        self.enemy.move()\n",
    "\n",
    "        if self.RETURN_IMAGE:\n",
    "            new_observation = np.array(self.get_image())/255\n",
    "        else:\n",
    "            new_observation = (self.player - self.food)+(self.player - self.enemy)\n",
    "\n",
    "        if self.player == self.food :\n",
    "            reward = self.FOOD_REWARD\n",
    "        elif self.player == self.enemy :\n",
    "            reward = self.ENEMY_PENALITY\n",
    "        else:\n",
    "            reward = self.MOVE_PENALITY\n",
    "\n",
    "        done = False\n",
    "        if self.player == self.food or self.player == self.enemy or self.episode_step>=200:\n",
    "            done = True\n",
    "        \n",
    "        return new_observation,reward,done,{}\n",
    "    \n",
    "    def render(self,mode='human'):\n",
    "        img = self.get_image()       \n",
    "        img = img.resize((800,800))\n",
    "        cv2.imshow('Predator',np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE,self.SIZE,3), dtype=np.uint8)\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]\n",
    "        img = Image.fromarray(env,'RGB')\n",
    "        return img\n",
    "\n",
    "    def get_qtable(self,qtable_name=None):\n",
    "        if qtable_name is None:\n",
    "            q_table = {}\n",
    "            for x1 in range(-self.SIZE+1, self.SIZE):\n",
    "                for y1 in range(-self.SIZE+1, self.SIZE):\n",
    "                    for x2 in range(-self.SIZE+1, self.SIZE):\n",
    "                        for y2 in range(-self.SIZE+1, self.SIZE):\n",
    "                            q_table[(x1,y1,x2,y2)] = [np.random.uniform(-5,0) for i in range(self.ACTION_SPACE_VALUES)]\n",
    "        else:\n",
    "            with open(qtable_name,'rb') as f:\n",
    "                q_table=pickle.load(f)\n",
    "        return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "env = envCube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(status,nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',input_shape=(1,) + status))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))    \n",
    "    model.add(Dense(nb_actions,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 1, 8, 8, 32)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 6, 6, 32)       9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                36896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 48,393\n",
      "Trainable params: 48,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env.OBSERVATION_SPACE_VALUES,env.ACTION_SPACE_VALUES)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model,nb_actions):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "                   enable_double_dqn=True,target_model_update=5000, policy=policy)\n",
    "    dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_duel_agent(model,nb_actions):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1) \n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=500000)\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "               enable_dueling_network=True, dueling_type='avg',policy=policy)\n",
    "    dqn.compile(Adam(learning_rate=1e-4))\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_duel = build_duel_agent(model,env.ACTION_SPACE_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -2.8629\n",
      "156 episodes - episode_reward: -183.192 [-491.000, 25.000] - loss: 612.513 - mean_q: 93.782 - mean_eps: 0.990\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      " 8167/10000 [=======================>......] - ETA: 45s - reward: -2.4803done, took 420.723 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9a119c2e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_weights_filename = './models_duel/dqn_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=10000)]\n",
    "dqn_duel.fit(env, nb_steps=1000000, visualize=False, verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model,env.ACTION_SPACE_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 26:03 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangsy/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2464: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 106s 11ms/step - reward: -2.6900\n",
      "137 episodes - episode_reward: -196.095 [-493.000, 24.000] - loss: 170.642 - mae: 5.091 - mean_q: -0.080\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: -3.2347\n",
      "137 episodes - episode_reward: -236.255 [-484.000, 25.000] - loss: 231.204 - mae: 11.132 - mean_q: -6.692\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: -2.2571\n",
      "117 episodes - episode_reward: -192.709 [-493.000, 25.000] - loss: 209.880 - mae: 14.052 - mean_q: -10.550\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: -2.6224\n",
      "142 episodes - episode_reward: -184.704 [-483.000, 24.000] - loss: 201.401 - mae: 14.187 - mean_q: -11.529\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -2.8928\n",
      "138 episodes - episode_reward: -209.594 [-489.000, 25.000] - loss: 198.439 - mae: 17.624 - mean_q: -15.747\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 239s 24ms/step - reward: -2.4456\n",
      "135 episodes - episode_reward: -181.407 [-480.000, 25.000] - loss: 208.235 - mae: 23.469 - mean_q: -22.667\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: -2.2766\n",
      "127 episodes - episode_reward: -178.228 [-499.000, 25.000] - loss: 193.905 - mae: 25.916 - mean_q: -25.542\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 292s 29ms/step - reward: -2.7940\n",
      "123 episodes - episode_reward: -227.520 [-492.000, 24.000] - loss: 185.358 - mae: 27.315 - mean_q: -26.847\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: -2.7186\n",
      "128 episodes - episode_reward: -212.750 [-480.000, 25.000] - loss: 182.188 - mae: 30.492 - mean_q: -30.195\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: -2.0218\n",
      "done, took 2140.278 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9e2d49a00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('double-dqn_weights_R2.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -87.000, steps: 113\n",
      "Episode 3: reward: -24.000, steps: 50\n",
      "Episode 4: reward: 25.000, steps: 1\n",
      "Episode 5: reward: -104.000, steps: 130\n",
      "Episode 6: reward: -9.000, steps: 35\n",
      "Episode 7: reward: -10.000, steps: 36\n",
      "Episode 8: reward: -20.000, steps: 46\n",
      "Episode 9: reward: -51.000, steps: 77\n",
      "Episode 10: reward: 11.000, steps: 15\n",
      "Episode 11: reward: -200.000, steps: 200\n",
      "Episode 12: reward: -84.000, steps: 110\n",
      "Episode 13: reward: -348.000, steps: 49\n",
      "Episode 14: reward: -200.000, steps: 200\n",
      "Episode 15: reward: -404.000, steps: 105\n",
      "Episode 16: reward: 10.000, steps: 16\n",
      "Episode 17: reward: -303.000, steps: 4\n",
      "Episode 18: reward: -200.000, steps: 200\n",
      "Episode 19: reward: -200.000, steps: 200\n",
      "Episode 20: reward: -200.000, steps: 200\n",
      "-129.9\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=20, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dqn,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd9a044e250> and <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd9a06b6a90>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd9a06b6a90> and <tensorflow.python.keras.layers.core.Flatten object at 0x7fd9f1c0e5e0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7fd9a06d7d90> and <tensorflow.python.keras.layers.core.Dense object at 0x7fd9a06e9610>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7fd9a06e9610> and <tensorflow.python.keras.layers.core.Dense object at 0x7fd9a06e9c10>).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer weight shape (32, 9) not compatible with provided weight shape (32, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7872e86765af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/dqn_weights_imageEnv_810000.h5f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mupdate_target_model_hard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0mref_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m           raise ValueError(\n\u001b[0m\u001b[1;32m   1299\u001b[0m               \u001b[0;34m'Layer weight shape %s not compatible with provided weight '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m               'shape %s' % (ref_shape, weight_shape))\n",
      "\u001b[0;31mValueError\u001b[0m: Layer weight shape (32, 9) not compatible with provided weight shape (32, 10)"
     ]
    }
   ],
   "source": [
    "dqn.load_weights('./models/dqn_weights_imageEnv_810000.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangsy/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2464: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: 16.000, steps: 10\n",
      "Episode 3: reward: 0.000, steps: 26\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -75.000, steps: 101\n",
      "Episode 6: reward: 12.000, steps: 14\n",
      "Episode 7: reward: 9.000, steps: 17\n",
      "Episode 8: reward: 4.000, steps: 22\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "Episode 10: reward: -51.000, steps: 77\n",
      "-68.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
